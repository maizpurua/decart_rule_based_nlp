{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now it's time to work on the final group project.  All of the pieces we've learned are integrated into this initial template to work on improving F1 measure on classification of pneumonia evidence\n",
    "\n",
    "# You are welcome to spend your time however you'd like but here are a few ideas of how to improve your system:\n",
    "* Improve targets.  Are there any False Negatives your system is missing?  Are there regular expressions that would help?\n",
    "* Improve modifiers.  Not all modifiers typically used in practice are the modifiers starter file.  Are there some to add?  Do some existing modifiers cause problems in your processing?  They can be changed or removed.\n",
    "* Improve document classification rules.  What rules work best?  What is the best \"default\" classification?\n",
    "* Consider handling of document \"sections\".  Are there certain headers or subsections which are more or less likely to contain evidence?  You could modify your own \"markup\" function to do this or you could add Modifiers to do this in some cases\n",
    "\n",
    "# Also before we get going, a few Pro Tips:\n",
    "* Remember that pyConText files need to be tab delimited.  If you edit these files in JupyterHub, it might be difficult to see the tabs and if you press \"TAB\" you will actually get spaces, so try to use Copy-and-Paste\n",
    "* Classification rules and modifiers are difficult.  Don't be afraid to ask for help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some packages\n",
    "import os\n",
    "import pyConTextNLP\n",
    "from pyConTextNLP import pyConText\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import radnlp.view as rview\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display, HTML, Image\n",
    "import ipywidgets\n",
    "# And also our utilities for this class\n",
    "\n",
    "from nlp_pneumonia_utils import read_doc_annotations\n",
    "from nlp_pneumonia_utils import mark_document_with_html\n",
    "from DocumentClassifier import DocumentClassifier\n",
    "from visual import view_pycontext_output\n",
    "from visual import snippets_markup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading annotations from file : data/test_v2.zip\n",
      "Opening local file : data/test_v2.zip\n",
      "Total Annotated Documents : 30\n"
     ]
    }
   ],
   "source": [
    "annotated_doc_map = read_doc_annotations('data/test_v2.zip')\n",
    "\n",
    "# let's also use a simple list of documents as well as this map\n",
    "annotated_docs = list(annotated_doc_map.values())\n",
    "\n",
    "print('Total Annotated Documents : {0}'.format(len(annotated_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct our Document Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_doc_type='PNEUMONIA_DOC_YES'\n",
    "TARGETS_FILE_PATH = 'KB/pneumonia_targets.yml'\n",
    "MODIFIERS_FILE_PATH = 'KB/pneumonia_modifiers.yml'\n",
    "FEATURE_INFERENCER_FILE_PATH = 'KB/featurer_inferences.csv'\n",
    "DOC_INFERENCER_FILE_PATH = 'KB/doc_inferences.csv'\n",
    "# clear just in case files/regular expressions have been updated\n",
    "classifier = DocumentClassifier(TARGETS_FILE_PATH, MODIFIERS_FILE_PATH,\n",
    "                               FEATURE_INFERENCER_FILE_PATH, DOC_INFERENCER_FILE_PATH,\n",
    "                               {pos_doc_type})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's attempt some predictions\n",
    "* You will do a lot of iterations modifying content and then coming back here to check performane\n",
    "* Remember : the prediction function passed here passes in a string (text) and returns a 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to evaluate against reference standards...\n",
      "Precision : 0.867\n",
      "Recall :    0.929\n",
      "F1:         0.897\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1   0\n",
       "Actual           \n",
       "1          13   1\n",
       "0           2  14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_false_negatives, current_false_positives, measurements,confusion_matrix_df = classifier.eval(annotated_doc_map)\n",
    "print(measurements)\n",
    "display(confusion_matrix_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development of your system:\n",
    "* We have found the tools below for highlighting and graphing False Positives and False Negatives to be very useful.  We've provided them below in case it helps you as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For False Negatives, it's most useful to see the expert span annotations for positive pneumonia evidence to see if there may be targets that should be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><div style=\"background-color:#f9f9f9;padding-left:10px;width: 877px; \"><table width=100% ><col style=\"width:25%\"><col style=\"width:75%\"></div><tr><th style=\"text-align:center\">document name</th><th style=\"text-align:center\">Snippets</th></table></div><div style=\"background-color:#f9f9f9;padding:10px; width: 900px; height: 400px; overflow-y: scroll;\"><table width=100% ><col style=\"width:25%\"><col style=\"width:75%\"><tr><td style=\"text-align:left\">subject_id_157_hadm_id_26180</td><td></td></tr><tr><td></td><td style=\"text-align:left\">ung is incompletely imaged\n",
       "     on this study and <span style=\"color: blue;\">there is a questionable area of abnormality partially\n",
       "     obscuring the mid portion of the right hemidiaphragm</span>, incompletely evaluated.\n",
       "     \n",
       "     IMPRESSION:  </td></tr><tr><td style=\"text-align:left\">subject_id_5472_hadm_id_11987</td><td></td></tr><tr><td></td><td style=\"text-align:left\">id SVC. \n",
       "     There is no apparent pneumothorax.  <span style=\"color: blue;\">A right IJ line, NGT, and ETT are\n",
       "     unchanged as are the parenchymal changes in the lungs</span> compared to the earlier\n",
       "     chest x-ray this mor</td></tr><tr><td style=\"text-align:left\">subject_id_9082_hadm_id_29395</td><td></td></tr><tr><td></td><td style=\"text-align:left\">tient with seizure.\n",
       "     \n",
       "     Low lung volumes.  <span style=\"color: blue;\">Bilateral basilar opacities,</span> considerably larger at the\n",
       "     left base than at</td></tr><tr><td></td><td style=\"text-align:left\">hyroid.\n",
       "     \n",
       "     IMPRESSION:  Lung volumes with <span style=\"color: blue;\">bilateral basilar opacities.</span>\n",
       "     \n",
       "     Question substernal thyroid enlargemen</td></tr></table></div></html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fn_docs=dict((k, v) for k, v in annotated_doc_map.items() if k in current_false_negatives)\n",
    "display(HTML(snippets_markup(fn_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No documents to view.\n"
     ]
    }
   ],
   "source": [
    "# It may be still useful to check if your pyConText exclude unexpected annotations to result the false negative\n",
    "\n",
    "fn_docs = dict((k,v) for k, v in classifier.saved_markups_map.items() if k in current_false_negatives)\n",
    "view_pycontext_output(fn_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For False Positives, it's most useful to see a pyConText graph since there may need to be modifiers adjusted so that targets can be properly utilized in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1defe85ca4cc407aa5e66d255d963480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=0), Output()), _dom_classes=('widget-interact',)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fp_docs = dict((k,v) for k, v in classifier.saved_markups_map.items() if k in current_false_positives)\n",
    "view_pycontext_output(fp_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TEST SET Evaluation \n",
    "* We've been waiting for the test set.  It will not be available until the morning of the final class session.\n",
    "* At that time, you can uncomment this code and make any changes to it as instructed by the class instructors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading annotations from file : data/test_v2.zip\n",
      "Opening local file : data/test_v2.zip\n",
      "Total Test Documents : 30\n",
      "****************\n",
      "Performance for Classifier on TEST set :\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calculate_prediction_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-310-affd2345df68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'****************'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Performance for Classifier on TEST set :'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcalculate_prediction_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_prediction_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "test_doc_map = read_doc_annotations('data/test_v2.zip')\n",
    "\n",
    "# let's also use a simple list of documents as well as this map\n",
    "test_docs = list(test_doc_map.values())\n",
    "\n",
    "print('Total Test Documents : {0}'.format(len(test_docs)))\n",
    "\n",
    "# and now let's check performance on the TEST set...\n",
    "print('****************')\n",
    "print('Performance for Classifier on TEST set :')\n",
    "calculate_prediction_metrics(test_docs, docClassifier.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>This material presented as part of the DeCART Data Science for the Health Science Summer Program at the University of Utah in 2019.<br/>\n",
    "Presenters : Dr. Wendy Chapman, Kelly Peterson, Alec Chapman, Jianlin Shi <br> Acknowledgement: Many thanks to Olga Patterson because part of the materials are adopted from his previous work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
